# Code examples taken and adapted from :
# https://www.scaler.com/topics/nlp/bert-next-sentence-prediction/
#
# Example of BERT's Next Sentence Prediction
# 
# The transformers library must be installed for this code :
# pip install transformers
#
# PyTorch must be installed for this code :
# pip install torch
#
# It is recommended that you use the supplied pt_bert environment, which
# will save you needed to manually install the correct packages.
#
# Important!  The first time you run this it will need to download some large
# files.  Be aware of this if you're running it for the first time,
# particularly if you have a slower internet connection.

# We'll import the necessary modules from the transformers library so we can
# explore the Next Sentence Prediction aspect of BERT.  We'll also import torch
# (this is PyTorch - the main alternative to TensorFlow)
# The transformers library was developed by Hugging Face : 
# https://huggingface.co/docs/transformers/index
from transformers import BertTokenizer, BertForNextSentencePrediction
import torch

# We'll set up a Tokenizer (which splits text into tokens) and a BERT model,
# and we'll use the ones from the original BERT model (bert-base-uncased) for 
# both
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')

# Let's set up two sentences
sentence_A = "Dan bought a new computer. \
    It was an extremely fast Alienware PC."
sentence_B = "He was extremely impressed with the speeed of it.\
    It was much faster than his old one."

# Alternative pair of sentences where sentence B does not follow from sentence A
#sentence_A = "Dan had a late lunch on Tuesday."
#sentence_B = "Donkeys are not native to Canada."

# We'll tokenize the above sentences using the tokenizer we set up.
# return_tensors='pt' means return the tensors in the format needed for PyTorch.
# return_tensors='tf' would return them in TensorFlow format (BERT can be used
# with either).  Here we're using PyTorch.
tokenized = tokenizer(sentence_A, sentence_B, return_tensors='pt')

# The tokenizer returns a dictionary (which we've named tokenized).  Let's have
# a look at the keys in this dictionary
print (tokenized.keys())

# We can see from the above that we have three keys :
# 'input_ids' - these are the tokens (as numbers) representing words,
# punctuation etc
# 'token_type_ids' - this indicates either a 0 or a 1 for each token, depending
# on the "segment" to which the token belongs.  So in this case, we've got two
# sentences in the pair - tokens with a type id of 0 belong to the first
# sentence, and those with a type id of 1 belong to the second sentence.
# 'attention_mask' - this basically indicates which tokens BERT needs to pay
# attention to (value of 1) and which it can ignore (value of 0).  You'll see
# all the values are 1 here.

# Let's look at the values for each key in our dictionary (tokenized)
print (tokenized['input_ids'])
print (tokenized['token_type_ids'])
print (tokenized['attention_mask'])

# You'll see from the above that the tokenizer has merged our pair of sentences
# together into a single set of tensors (but we can still tell to which sentence
# each token belonged from the token_type_ids).  You'll also see two [SEP]
# tokens in there (a separator token) - this has the ID 102.  One is in between
# the two sentences, and the other at the end of the pair.

# Now we'll set up a new Tensor named 'labels' that will indicate whether or
# not the pair of sentences represents a true pair (ie sentence B is predicted
# to follow sentence A) or a false pair (ie sentence B is NOT predicted to
# follow sentence A).  Slightly confusingly, a true pair is represented with a
# value of 0, and a false pair with a value of 1...
# We set it up as a PyTorch LongTensor - this is a 64-bit integer Tensor (see :
# https://pytorch.org/docs/stable/tensors.html).  You can use 
# labels = torch.cuda.LongTensor 
# to use a GPU version (if you have a CUDA-enabled GPU set up)
labels = torch.LongTensor([0])

# Let's generate our prediction
# We set up our model to make the prediction.  We use ** in front of tokenized
# (the dictionary generated by the tokenizer) here to indicate it represents 
# **kwargs (keyword arguments).  You can read more about *args and **kwargs
# here : https://realpython.com/python-kwargs-and-args/ but basically, *args
# are used when you want to pass in a varying number of inputs (arguments) to a 
# function where the position of each input matters, whereas **kwargs does the 
# same thing but uses named arguments (rather than positional arguments).  Here,
# we have a dictionary that we pass in containing named arguments (each key
# in the dictionary represents the name of the argument we're passing in to
# model)
# We also supply the labels tensor as an input that we set up earlier (this
# one will store the 0 or 1 labels).
# The model will output losses (error) and logits (the output of a Neural
# Network before it has passed through an activation function to normalise it)
predict = model(**tokenized, labels=labels)

# with our model set up, we'll use it to generate a prediction of whether the
# pair of sentences we've provided represents a True Pair or a False Pair.  We
# use PyTorch's argmax function to do this - it will find the indices of the
# maximum value of the input tensor.  Don't worry too much about this - just
# know that it'll give us a 0 for a True Pair or a 1 for a False Pair.
prediction = torch.argmax(predict.logits)

# Print an appropriate message depending on the prediction
if prediction == 0:
    print ("True Pair")
else:
    print ("False Pair")

